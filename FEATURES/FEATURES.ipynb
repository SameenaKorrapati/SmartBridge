{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SUMMARY**"
      ],
      "metadata": {
        "id": "W1rkLC0mMCfR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXX5e-LQDL3Z",
        "outputId": "faaab9e8-4b6b-4818-c966-61b0f8fc3ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install language-tool-python\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Ufel-gQrCtTj",
        "outputId": "bd81cb2c-6a7b-4dcf-e06e-0bbab1d4717b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "#Input your text for summarizing below:\n",
        "\n",
        "text = \"\"\"\"\"\"\n",
        "\n",
        "#Next, you need to tokenize the text:\n",
        "\n",
        "stopWords = set(stopwords.words(\"english\"))\n",
        "words = word_tokenize(text)\n",
        "# Now, you will need to create a frequency table to keep a score of each word:\n",
        "\n",
        "freqTable = dict()\n",
        "for word in words:\n",
        "\tword = word.lower()\n",
        "\tif word in stopWords:\n",
        "\t\tcontinue\n",
        "\tif word in freqTable:\n",
        "\t\tfreqTable[word] += 1\n",
        "\telse:\n",
        "\t\tfreqTable[word] = 1\n",
        "\n",
        "#Next, create a dictionary to keep the score of each sentence:\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "sentenceValue = dict()\n",
        "\n",
        "for sentence in sentences:\n",
        "\tfor word, freq in freqTable.items():\n",
        "\t\tif word in sentence.lower():\n",
        "\t\t\tif sentence in sentenceValue:\n",
        "\t\t\t\tsentenceValue[sentence] += freq\n",
        "\t\t\telse:\n",
        "\t\t\t\tsentenceValue[sentence] = freq\n",
        "\n",
        "\tsumValues = 0\n",
        "\tfor sentence in sentenceValue:\n",
        "\t\tsumValues += sentenceValue[sentence]\n",
        "\n",
        "#Now, we define the average value from the original text as such:\n",
        "\n",
        "average = int(sumValues / len(sentenceValue))\n",
        "\n",
        "#And lastly, we need to store the sentences into our summary:\n",
        "\n",
        "summary = ''\n",
        "\n",
        "for sentence in sentences:\n",
        "\n",
        "\tif (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
        "\t\tsummary += \" \" + sentence\n",
        "print(summary)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DUPLICATE ELIMINATION**"
      ],
      "metadata": {
        "id": "3bFtgkecMMvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import deque\n",
        "\n",
        "# Load the Universal Sentence Encoder\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
        "\n",
        "# Initialize a deque to store the recent sentences\n",
        "recent_sentences = deque(maxlen=10)\n",
        "\n",
        "def remove_duplicate_sentences(sentence):\n",
        "    # Embed the sentence using USE\n",
        "    sentence_embedding = embed([sentence])[0]\n",
        "\n",
        "    # Calculate cosine similarity between the sentence and recent sentences\n",
        "    similarities = []\n",
        "    for recent_sentence in recent_sentences:\n",
        "        recent_sentence_embedding = embed([recent_sentence])[0]\n",
        "        similarity = cosine_similarity([sentence_embedding], [recent_sentence_embedding])[0][0]\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    # Check if the sentence is similar to any recent sentences\n",
        "    if len(similarities) > 0 and max(similarities) > 0.8:\n",
        "        return None  # Sentence is a duplicate\n",
        "    else:\n",
        "        recent_sentences.append(sentence)\n",
        "        return sentence\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"ml is very useful.\"\n",
        "sentence2 = \"This is another sample sentence.\"\n",
        "sentence3 = \"ml is useful.\"  # Duplicate of sentence1\n",
        "sentence4 = \"This is yet another sentence.\"\n",
        "\n",
        "cleaned_sentence1 = remove_duplicate_sentences(sentence1)\n",
        "print(cleaned_sentence1)  # Output: \"This is a sample sentence.\"\n",
        "\n",
        "cleaned_sentence2 = remove_duplicate_sentences(sentence2)\n",
        "print(cleaned_sentence2)  # Output: \"This is another sample sentence.\"\n",
        "\n",
        "cleaned_sentence4 = remove_duplicate_sentences(sentence4)\n",
        "print(cleaned_sentence4)  # Output: \"This is yet another sentence.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybsPeykJaJe6",
        "outputId": "becb773e-2a0f-4ced-99cb-ca7e653fb4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ml is very useful.\n",
            "This is another sample sentence.\n",
            "This is yet another sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOUTUBE LINK**"
      ],
      "metadata": {
        "id": "0TGC_92oMXe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "def get_video_links(topic, max_results=10):\n",
        "    api_key = \"AIzaSyB81cBK67-ta40qSSYhP05O7GuyS38_fRY\"\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    search_response = youtube.search().list(\n",
        "        q=topic,\n",
        "        part=\"id\",\n",
        "        maxResults=max_results,\n",
        "        type=\"video\"\n",
        "    ).execute()\n",
        "\n",
        "    video_links = []\n",
        "    for search_result in search_response.get(\"items\", []):\n",
        "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
        "            video_id = search_result[\"id\"][\"videoId\"]\n",
        "            video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "            video_links.append(video_link)\n",
        "\n",
        "    return video_links\n",
        "\n",
        "topic = input(\"enter the topic:\")\n",
        "links = get_video_links(topic, max_results=10)\n",
        "for link in links:\n",
        "    print(link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWqu14HnE6fZ",
        "outputId": "b8467584-32d9-4360-9d0e-b20d97e39c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the topic:cybersecurity\n",
            "https://www.youtube.com/watch?v=2zdIvSgRRv8\n",
            "https://www.youtube.com/watch?v=xQZb-cumdR4\n",
            "https://www.youtube.com/watch?v=YeWYlp9JP6g\n",
            "https://www.youtube.com/watch?v=FPb19YINif4\n",
            "https://www.youtube.com/watch?v=jq_LZ1RFPfU\n",
            "https://www.youtube.com/watch?v=inWWhr5tnEA\n",
            "https://www.youtube.com/watch?v=DRJic8vCodE\n",
            "https://www.youtube.com/watch?v=4CuXNs6SboU\n",
            "https://www.youtube.com/watch?v=MlYUxdcf6v0\n",
            "https://www.youtube.com/watch?v=_DVVNOGYtmU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRAMMER CORRECTION**"
      ],
      "metadata": {
        "id": "1YIwLXTsO4v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gingerit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACVvGYYJOd6O",
        "outputId": "a7544576-6132-4948-c85b-97fc1e1e5185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gingerit\n",
            "  Downloading gingerit-0.9.0-py3-none-any.whl (3.4 kB)\n",
            "Collecting cloudscraper<2.0.0,>=1.2.66 (from gingerit)\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.10/dist-packages (from cloudscraper<2.0.0,>=1.2.66->gingerit) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from cloudscraper<2.0.0,>=1.2.66->gingerit) (2.27.1)\n",
            "Collecting requests-toolbelt>=0.9.1 (from cloudscraper<2.0.0,>=1.2.66->gingerit)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.2->cloudscraper<2.0.0,>=1.2.66->gingerit) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.2->cloudscraper<2.0.0,>=1.2.66->gingerit) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.2->cloudscraper<2.0.0,>=1.2.66->gingerit) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.2->cloudscraper<2.0.0,>=1.2.66->gingerit) (3.4)\n",
            "Installing collected packages: requests-toolbelt, cloudscraper, gingerit\n",
            "Successfully installed cloudscraper-1.2.71 gingerit-0.9.0 requests-toolbelt-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gingerit.gingerit import GingerIt\n",
        "\n",
        "def correct_grammar(paragraph):\n",
        "    parser = GingerIt()\n",
        "    corrected_result = parser.parse(paragraph)\n",
        "    corrected_paragraph = corrected_result['result']\n",
        "\n",
        "    return corrected_paragraph\n",
        "\n",
        "paragraph = \"He is a engineer and he enjoy playing guitar.\"\n",
        "corrected_paragraph = correct_grammar(paragraph)\n",
        "print(corrected_paragraph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoo4EQ81OkSM",
        "outputId": "1e863c63-7525-4b09-ce54-a388a156cbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He is an engineer and he enjoys playing guitar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HANDWRITTING -> TEXT**"
      ],
      "metadata": {
        "id": "4BXaMU4ACWpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL41dOjtK5UB",
        "outputId": "1cd2449a-2d25-46cc-90bb-fb9ad59c2b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m2.4/2.9 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.15.2+cu118)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.8.0.74)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.3)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.1)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (813 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.9/813.9 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5->easyocr) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->easyocr) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->easyocr) (16.0.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi->easyocr) (1.16.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2023.7.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (23.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Installing collected packages: pyclipper, ninja, python-bidi, easyocr\n",
            "Successfully installed easyocr-1.7.0 ninja-1.11.1 pyclipper-1.3.0.post4 python-bidi-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import easyocr\n",
        "import editdistance\n",
        "\n",
        "# Function to calculate the Word Error Rate (WER) between two strings\n",
        "def calculate_wer(ground_truth, recognized_text):\n",
        "    # Convert both strings to lowercase for case-insensitive comparison\n",
        "    ground_truth = ground_truth.lower()\n",
        "    recognized_text = recognized_text.lower()\n",
        "\n",
        "    # Split both strings into words\n",
        "    ground_truth_words = ground_truth.split()\n",
        "    recognized_words = recognized_text.split()\n",
        "\n",
        "    # Calculate the number of substitutions, insertions, and deletions\n",
        "    dist = editdistance.eval(ground_truth_words, recognized_words)\n",
        "    wer = dist / max(len(ground_truth_words), len(recognized_words))\n",
        "\n",
        "    return wer\n",
        "\n",
        "# Function to recognize handwritten text using EasyOCR\n",
        "def recognize_handwritten_text(image_path):\n",
        "    try:\n",
        "        reader = easyocr.Reader(['en'])  # Initialize the reader with English language support\n",
        "        result = reader.readtext(image_path, detail=0)\n",
        "        recognized_text = ' '.join(result)\n",
        "        return recognized_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"/content/WETEST8.jpg\"  # Update with the image path in Google Colab\n",
        "    ground_truth_text = \"SWETHA BALAJI\"\n",
        "    recognized_text = recognize_handwritten_text(image_path)\n",
        "\n",
        "    if recognized_text:\n",
        "        print(\"Recognized Text:\")\n",
        "        print(recognized_text)\n",
        "\n",
        "        # Calculate and print the Word Error Rate (WER)\n",
        "        wer = calculate_wer(ground_truth_text, recognized_text)\n",
        "        print(\"Word Error Rate (WER): {:.2f}\".format(wer))\n",
        "    else:\n",
        "        print(\"OCR Failed. Unable to recognize text from the image.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyXYNVNpOWoF",
        "outputId": "0f6f5298-3064-4eac-fa8f-afbe8a998432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognized Text:\n",
            "SwETHA BALA TI\n",
            "Word Error Rate (WER): 0.67\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}